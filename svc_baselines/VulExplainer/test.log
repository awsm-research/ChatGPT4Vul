07/01/2023 11:32:42 - WARNING - __main__ -   device: cuda:1, n_gpu: 1
07/01/2023 11:32:49 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../../data/big_vul/train.csv', output_dir='./saved_models', use_logit_adjustment=False, use_hard_distil=False, tau=1.2, alpha=0.7, beta=0.8, model_type='bert', block_size=512, eval_data_file='../../data/big_vul/val.csv', test_data_file='../../data/big_vul/test.csv', model_name='soft_distil_model.bin', model_name_or_path='microsoft/codebert-base', config_name='', use_non_pretrained_model=False, tokenizer_name='microsoft/codebert-base', code_length=256, do_train=False, do_eval=False, do_test=True, evaluate_during_training=True, do_token_level_eval=False, reasoning_method='attention', train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, warmup_steps=0, seed=123456, epochs=50, n_gpu=1, device=device(type='cuda', index=1))
  0%|          | 0/864 [00:00<?, ?it/s]  4%|▍         | 35/864 [00:00<00:02, 348.32it/s] 10%|▉         | 85/864 [00:00<00:01, 432.44it/s] 17%|█▋        | 145/864 [00:00<00:01, 506.26it/s] 25%|██▍       | 213/864 [00:00<00:01, 570.74it/s] 31%|███▏      | 271/864 [00:00<00:01, 527.52it/s] 38%|███▊      | 325/864 [00:00<00:01, 526.31it/s] 45%|████▌     | 389/864 [00:00<00:00, 553.75it/s] 53%|█████▎    | 462/864 [00:00<00:00, 604.11it/s] 61%|██████    | 523/864 [00:00<00:00, 552.94it/s] 67%|██████▋   | 580/864 [00:01<00:00, 510.45it/s] 73%|███████▎  | 634/864 [00:01<00:00, 515.63it/s] 80%|███████▉  | 687/864 [00:01<00:00, 512.86it/s] 86%|████████▌ | 739/864 [00:01<00:00, 513.89it/s] 92%|█████████▏| 798/864 [00:01<00:00, 505.29it/s] 99%|█████████▊| 852/864 [00:01<00:00, 513.75it/s]100%|██████████| 864/864 [00:01<00:00, 516.38it/s]
07/01/2023 11:32:52 - INFO - __main__ -   ***** Running Test *****
07/01/2023 11:32:52 - INFO - __main__ -     Num examples = 864
07/01/2023 11:32:52 - INFO - __main__ -     Batch size = 8
07/01/2023 11:32:58 - INFO - __main__ -   ***** Test results *****
07/01/2023 11:32:58 - INFO - __main__ -     test_accuracy = 0.6343
07/01/2023 11:32:58 - INFO - __main__ -     weighted_f1 = 0.6293
